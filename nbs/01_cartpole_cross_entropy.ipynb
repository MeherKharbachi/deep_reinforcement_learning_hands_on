{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp cartpole_cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f21c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a48f02",
   "metadata": {},
   "source": [
    "Our model's core is a one-hidden-layer NN, with rectified linear unit (ReLU) and 128 hidden neurons (which is absolutely\n",
    "arbitrary).\n",
    "\n",
    "the count of neurons in the hidden layer is 128, the count of episodes we play on every iteration (16), and the\n",
    "percentile of episodes' total rewards that we use for \"elite\" episode filtering. We will\n",
    "take the 70th percentile, which means that we will leave the top 30% of episodes\n",
    "sorted by reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f998a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5054301b",
   "metadata": {},
   "source": [
    "Neural Network class that takes a single observation from the\n",
    "environment as an input vector and outputs a number for every action we can\n",
    "perform. The output from the NN is a probability distribution over actions, so\n",
    "a straightforward way to proceed would be to include softmax nonlinearity after\n",
    "the last layer.\n",
    "\n",
    "In addition we define two helper classes : \n",
    "\n",
    " - EpisodeStep: This will be used to represent one single step that our agent made in the episode, and it stores the observation from the environment and what action the agent completed. We will use episode steps from \"elite\" episodes as training data.\n",
    "\n",
    "- Episode: This is a single episode stored as total undiscounted reward and a collection of EpisodeStep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8159dd1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a7a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "Episode = namedtuple(\"Episode\", field_names=[\"reward\", \"steps\"])\n",
    "EpisodeStep = namedtuple(\"EpisodeStep\", field_names=[\"observation\", \"action\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dea12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        step = EpisodeStep(observation=obs, action=action)\n",
    "        episode_steps.append(step)\n",
    "        if is_done:\n",
    "            e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99262e19",
   "metadata": {},
   "source": [
    "The training accepts the environment (the Env class instance from the Gym library), our NN, and the count of episodes it should generate on every iteration.\n",
    "\n",
    "We also declare a reward counter for the current episode and its\n",
    "list of steps (the EpisodeStep objects). Then we reset our environment to obtain the\n",
    "first observation and create a softmax layer, which will be used to convert the NN's\n",
    "output to a probability distribution of actions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffcc04f",
   "metadata": {},
   "source": [
    "\n",
    "This function is at the core of the cross-entropy methodâ€”from the given batch\n",
    "of episodes and percentile value, it calculates a boundary reward, which is used\n",
    "to filter \"elite\" episodes to train on. To obtain the boundary reward, we will use\n",
    "NumPy's percentile function, which, from the list of values and the desired\n",
    "percentile, calculates the percentile's value. Then, we will calculate the mean\n",
    "reward, which is used only for monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f0949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for reward, steps in batch:\n",
    "        if reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, steps))\n",
    "        train_act.extend(map(lambda step: step.action, steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c46e9d0",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce9306b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.687, reward_mean=22.7, rw_bound=26.5\n",
      "1: loss=0.660, reward_mean=24.8, rw_bound=29.0\n",
      "2: loss=0.663, reward_mean=32.2, rw_bound=36.5\n",
      "3: loss=0.641, reward_mean=44.8, rw_bound=45.0\n",
      "4: loss=0.648, reward_mean=37.6, rw_bound=42.0\n",
      "5: loss=0.619, reward_mean=42.1, rw_bound=49.5\n",
      "6: loss=0.634, reward_mean=50.6, rw_bound=60.5\n",
      "7: loss=0.614, reward_mean=48.5, rw_bound=56.5\n",
      "8: loss=0.616, reward_mean=52.9, rw_bound=71.0\n",
      "9: loss=0.606, reward_mean=57.2, rw_bound=62.0\n",
      "10: loss=0.601, reward_mean=57.7, rw_bound=64.0\n",
      "11: loss=0.594, reward_mean=66.1, rw_bound=62.0\n",
      "12: loss=0.595, reward_mean=68.1, rw_bound=90.0\n",
      "13: loss=0.576, reward_mean=55.4, rw_bound=71.0\n",
      "14: loss=0.595, reward_mean=69.5, rw_bound=72.0\n",
      "15: loss=0.591, reward_mean=66.2, rw_bound=76.0\n",
      "16: loss=0.598, reward_mean=74.2, rw_bound=70.0\n",
      "17: loss=0.560, reward_mean=94.4, rw_bound=103.0\n",
      "18: loss=0.568, reward_mean=87.2, rw_bound=100.5\n",
      "19: loss=0.566, reward_mean=98.0, rw_bound=131.5\n",
      "20: loss=0.567, reward_mean=123.2, rw_bound=154.0\n",
      "21: loss=0.555, reward_mean=145.6, rw_bound=200.0\n",
      "22: loss=0.544, reward_mean=116.2, rw_bound=130.5\n",
      "23: loss=0.545, reward_mean=134.6, rw_bound=171.5\n",
      "24: loss=0.561, reward_mean=146.1, rw_bound=178.5\n",
      "25: loss=0.544, reward_mean=139.8, rw_bound=175.5\n",
      "26: loss=0.540, reward_mean=145.7, rw_bound=180.0\n",
      "27: loss=0.536, reward_mean=168.9, rw_bound=200.0\n",
      "28: loss=0.528, reward_mean=178.4, rw_bound=200.0\n",
      "29: loss=0.532, reward_mean=163.8, rw_bound=200.0\n",
      "30: loss=0.526, reward_mean=183.2, rw_bound=200.0\n",
      "31: loss=0.523, reward_mean=179.9, rw_bound=200.0\n",
      "32: loss=0.527, reward_mean=196.4, rw_bound=200.0\n",
      "33: loss=0.513, reward_mean=190.2, rw_bound=200.0\n",
      "34: loss=0.504, reward_mean=186.7, rw_bound=200.0\n",
      "35: loss=0.515, reward_mean=195.4, rw_bound=200.0\n",
      "36: loss=0.514, reward_mean=192.2, rw_bound=200.0\n",
      "37: loss=0.512, reward_mean=189.4, rw_bound=200.0\n",
      "38: loss=0.502, reward_mean=191.3, rw_bound=200.0\n",
      "39: loss=0.505, reward_mean=197.9, rw_bound=200.0\n",
      "40: loss=0.514, reward_mean=193.5, rw_bound=200.0\n",
      "41: loss=0.498, reward_mean=192.8, rw_bound=200.0\n",
      "42: loss=0.499, reward_mean=193.4, rw_bound=200.0\n",
      "43: loss=0.499, reward_mean=190.4, rw_bound=200.0\n",
      "44: loss=0.506, reward_mean=198.8, rw_bound=200.0\n",
      "45: loss=0.480, reward_mean=190.6, rw_bound=200.0\n",
      "46: loss=0.495, reward_mean=199.9, rw_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "# |export\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "\n",
    "    for iter_no, batch in enumerate(iterate_batches(\n",
    "            env, net, BATCH_SIZE)):\n",
    "        obs_v, acts_v, reward_b, reward_m = \\\n",
    "            filter_batch(batch, PERCENTILE)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        if reward_m > 199:\n",
    "            print(\"Solved!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef055c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
