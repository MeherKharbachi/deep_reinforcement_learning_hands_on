# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_cartpole_cross_entropy2.ipynb.

# %% auto 0
__all__ = ['model', 'trainer', 'Net', 'CrossEntropyMethod']

# %% ../nbs/02_cartpole_cross_entropy2.ipynb 1
import gym
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import pytorch_lightning as pl

class Net(nn.Module):
    def __init__(self, obs_size, hidden_size, n_actions):
        super(Net, self).__init__()
        self.net = nn.Sequential(
            nn.Linear(obs_size, hidden_size),
            nn.ReLU(),
            nn.Linear(hidden_size, n_actions),
        )

    def forward(self, x):
        return self.net(x)

class CrossEntropyMethod(pl.LightningModule):
    def __init__(self, obs_size, n_actions, hidden_size=128, batch_size=16, percentile=70):
        super(CrossEntropyMethod, self).__init__()
        self.net = Net(obs_size, hidden_size, n_actions)
        self.objective = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(params=self.net.parameters(), lr=0.01)
        self.batch_size = batch_size
        self.percentile = percentile
        self.env = gym.make("CartPole-v0")
    
    def forward(self, x):
        return self.net(x)

    def iterate_batches(self):
        batch = []
        while True:
            obs = self.env.reset()
            sm = nn.Softmax(dim=1)
            while True:
                obs_v = torch.FloatTensor([obs])
                act_probs_v = sm(self.net(obs_v))
                act_probs = act_probs_v.data.numpy()[0]
                action = np.random.choice(len(act_probs), p=act_probs)
                next_obs, reward, is_done, _ = self.env.step(action)
                batch.append((obs, action))
                if is_done:
                    break
                obs = next_obs
            if len(batch) == self.batch_size:
                yield batch
                batch = []

    def filter_batch(self, batch):
        rewards = [r for _, r in batch]
        reward_bound = np.percentile(rewards, self.percentile)
        reward_mean = float(np.mean(rewards))
        train_obs = [obs for obs, reward in batch if reward >= reward_bound]
        train_act = [act for _, act in batch if act >= reward_bound]
        train_obs_v = torch.FloatTensor(train_obs)
        train_act_v = torch.LongTensor(train_act)
        return train_obs_v, train_act_v, reward_bound, reward_mean

    def training_step(self, batch, batch_idx):
        obs_v, acts_v, reward_b, reward_m = self.filter_batch(batch)
        action_scores_v = self(obs_v)
        loss_v = self.objective(action_scores_v, acts_v)
        result = pl.TrainResult(loss=loss_v)
        result.log('loss', loss_v, prog_bar=True)
        result.log('reward_mean', reward_m, prog_bar=True)
        result.log('reward_bound', reward_b, prog_bar=True)
        return result

    def configure_optimizers(self):
        return self.optimizer

    def train_dataloader(self):
        return iter(self.iterate_batches())


# %% ../nbs/02_cartpole_cross_entropy2.ipynb 2
model = CrossEntropyMethod(obs_size=4, n_actions=2)
trainer = pl.Trainer(max_epochs=100)
trainer.fit(model)
