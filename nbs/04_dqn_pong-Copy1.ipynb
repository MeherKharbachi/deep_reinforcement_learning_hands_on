{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9173b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dqn_pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f21c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "import gym\n",
    "import pytorch_lightning as pl\n",
    "import d3rlpy\n",
    "import numpy as np\n",
    "import torch\n",
    "from d3rlpy.algos import DQN\n",
    "from d3rlpy.models.optimizers import OptimizerFactory\n",
    "from d3rlpy.online.buffers import ReplayBuffer\n",
    "from d3rlpy.online.explorers import LinearDecayEpsilonGreedy\n",
    "from torch.optim import Adam, Optimizer\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "from d3rlpy.metrics.scorer import evaluate_on_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a16d1b",
   "metadata": {},
   "source": [
    "The following class appears to define a DQN (Deep Q-Network) agent for training and playing games in a gym environment.\n",
    "\n",
    "The DQN algorithm is a model-based reinforcement learning technique used to learn a policy for selecting actions in a given state to maximize the long-term reward. It involves training a neural network to approximate the Q-function, which represents the expected future reward for each action at a given state.\n",
    "\n",
    "calc_loss: a function that calculates the loss for a given batch of experiences, using the current and target networks and the Bellman equation to calculate the expected Q-values for the current states and the Q-values from the target network for the next states. The loss is then calculated as the mean squared error between the expected and actual Q-values.\n",
    "\n",
    "ExperienceBuffer: a class that stores and samples a fixed-size buffer of Experience tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503874c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pytorch_lightning as pl\n",
    "import d3rlpy\n",
    "from d3rlpy.algos import DQN\n",
    "from d3rlpy.models.optimizers import OptimizerFactory\n",
    "from d3rlpy.online.buffers import ReplayBuffer\n",
    "from d3rlpy.online.explorers import LinearDecayEpsilonGreedy\n",
    "from torch.optim import Adam, Optimizer\n",
    "from pytorch_lightning import LightningModule, Trainer\n",
    "\n",
    "class PongModel(LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # create the environment\n",
    "        self.env = gym.make(\"PongNoFrameskip-v4\")\n",
    "        # create eval environment\n",
    "        self.eval_env = gym.make(\"PongNoFrameskip-v4\")\n",
    "        # set an optimizer fct\n",
    "        optim_factory = OptimizerFactory(Adam, weight_decay=1e-4)\n",
    "        # DQN model\n",
    "        self.dqn = DQN(\n",
    "            batch_size=32,\n",
    "            learning_rate=2.5e-4,\n",
    "            target_update_interval=100,\n",
    "            optim_factory=optim_factory,\n",
    "        )\n",
    "        # create the experience replay buffer\n",
    "        self.buffer = ReplayBuffer(maxlen=1000, env=self.env)\n",
    "        # epilon-greedy explorer\n",
    "        self.explorer = LinearDecayEpsilonGreedy(\n",
    "            start_epsilon=1.0, end_epsilon=0.1, duration=1000\n",
    "        )\n",
    "        model_parameters = {name: param for name, param in self.dqn.get_params().items()}\n",
    "        # create optimizer\n",
    "        self.optimizer = Adam(model_parameters, lr=2.5e-4)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Passes in a state x through the network and gets the q_values of each action as an output.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.dqn(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # step through environment with agent\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        loss = self.dqn.update(\n",
    "            states, actions, rewards, next_states, dones, self.buffer, self.explorer\n",
    "        )\n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer\n",
    "    \n",
    "    def __dataloader(self):\n",
    "        \"\"\"Initialize the Replay Buffer dataset used for retrieving experiences.\"\"\"\n",
    "        dataset = self.buffer.to_mdp_dataset\n",
    "        dataloader = DataLoader(dataset=dataset, batch_size=16)\n",
    "        return dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7148c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "optimizer can only optimize Tensors, but one of the params is str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [36]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# create an instance of PongModel\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mPongModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# create a Trainer instance\u001b[39;00m\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, gpus\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36mPongModel.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     33\u001b[0m model_parameters \u001b[38;5;241m=\u001b[39m {name: param \u001b[38;5;28;01mfor\u001b[39;00m name, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdqn\u001b[38;5;241m.\u001b[39mget_params()\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# create optimizer\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2.5e-4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/torch/optim/adam.py:90\u001b[0m, in \u001b[0;36mAdam.__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, amsgrad, foreach, maximize, capturable)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid weight_decay value: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(weight_decay))\n\u001b[1;32m     87\u001b[0m defaults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(lr\u001b[38;5;241m=\u001b[39mlr, betas\u001b[38;5;241m=\u001b[39mbetas, eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m     88\u001b[0m                 weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m     89\u001b[0m                 maximize\u001b[38;5;241m=\u001b[39mmaximize, foreach\u001b[38;5;241m=\u001b[39mforeach, capturable\u001b[38;5;241m=\u001b[39mcapturable)\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mAdam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/torch/optim/optimizer.py:54\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[0;34m(self, params, defaults)\u001b[0m\n\u001b[1;32m     51\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param_group \u001b[38;5;129;01min\u001b[39;00m param_groups:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_param_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Allows _cuda_graph_capture_health_check to rig a poor man's TORCH_WARN_ONCE in python,\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# which I don't think exists\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/72948\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warned_capturable_if_run_uncaptured \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/matchpred/lib/python3.9/site-packages/torch/optim/optimizer.py:293\u001b[0m, in \u001b[0;36mOptimizer.add_param_group\u001b[0;34m(self, param_group)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m param_group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 293\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer can only optimize Tensors, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut one of the params is \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtypename(param))\n\u001b[1;32m    295\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m param\u001b[38;5;241m.\u001b[39mis_leaf:\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt optimize a non-leaf Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: optimizer can only optimize Tensors, but one of the params is str"
     ]
    }
   ],
   "source": [
    "# create an instance of PongModel\n",
    "model = PongModel()\n",
    "\n",
    "# create a Trainer instance\n",
    "trainer = Trainer(max_epochs=1000)\n",
    "\n",
    "# start training\n",
    "trainer.fit(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deec287e",
   "metadata": {},
   "source": [
    "# Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f88767",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env_name: str,\n",
    "        eval_env_name: str,\n",
    "        buffer_maxlen: int,\n",
    "        start_epsilon: float,\n",
    "        end_epsilon: float,\n",
    "        duration: int,\n",
    "        batch_size: int,\n",
    "        learning_rate: float,\n",
    "        target_update_interval: int,\n",
    "        optim_factory: OptimizerFactory,\n",
    "        n_steps: int,\n",
    "        n_steps_per_epoch: int,\n",
    "        update_start_step: int,\n",
    "    ):\n",
    "        self.env_name = env_name\n",
    "        self.eval_env_name = eval_env_name\n",
    "        self.buffer_maxlen = buffer_maxlen\n",
    "        self.start_epsilon = start_epsilon\n",
    "        self.end_epsilon = end_epsilon\n",
    "        self.duration = duration\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.optim_factory = optim_factory\n",
    "        self.n_steps = n_steps\n",
    "        self.n_steps_per_epoch = n_steps_per_epoch\n",
    "        self.update_start_step = update_start_step\n",
    "\n",
    "        # create the environment\n",
    "        self.env = gym.make(self.env_name)\n",
    "        # create the evaluation environment\n",
    "        self.eval_env = gym.make(self.eval_env_name)\n",
    "        # create the DQN model\n",
    "        self.dqn = DQN(\n",
    "            batch_size=self.batch_size,\n",
    "            learning_rate=self.learning_rate,\n",
    "            target_update_interval=self.target_update_interval,\n",
    "        )\n",
    "        # create the experience replay buffer\n",
    "        self.buffer = ReplayBuffer(maxlen=self.buffer_maxlen, env=self.env)\n",
    "        # create the epsilon-greedy explorer\n",
    "        self.explorer = LinearDecayEpsilonGreedy(\n",
    "            start_epsilon=self.start_epsilon,\n",
    "            end_epsilon=self.end_epsilon,\n",
    "            duration=self.duration,\n",
    "        )\n",
    "\n",
    "    def train(self):\n",
    "        # start training\n",
    "        self.dqn.fit_online(\n",
    "            self.env,\n",
    "            self.buffer,\n",
    "            self.explorer,\n",
    "            n_steps=self.n_steps,\n",
    "            eval_env=self.eval_env,\n",
    "            n_steps_per_epoch=self.n_steps_per_epoch,\n",
    "            update_start_step=self.update_start_step,\n",
    "        )\n",
    "\n",
    "    def evaluate(self, render=True):\n",
    "        score = evaluate_on_environment(self.env, render=render)(self.dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8d4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-29 19:21.13 [info     ] Directory is created at d3rlpy_logs/DQN_online_20221229192113\n",
      "2022-12-29 19:21.13 [debug    ] Building model...\n",
      "2022-12-29 19:21.13 [debug    ] Model has been built.\n",
      "2022-12-29 19:21.13 [info     ] Parameters are saved to d3rlpy_logs/DQN_online_20221229192113/params.json params={'action_scaler': None, 'batch_size': 32, 'encoder_factory': {'type': 'default', 'params': {'activation': 'relu', 'use_batch_norm': False, 'dropout_rate': None}}, 'gamma': 0.99, 'generated_maxlen': 100000, 'learning_rate': 0.00025, 'n_critics': 1, 'n_frames': 1, 'n_steps': 1, 'optim_factory': {'optim_cls': 'Adam', 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False}, 'q_func_factory': {'type': 'mean', 'params': {'share_encoder': False}}, 'real_ratio': 1.0, 'reward_scaler': None, 'scaler': None, 'target_update_interval': 100, 'use_gpu': None, 'algorithm': 'DQN', 'observation_shape': (4,), 'action_size': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964858a5d8154e96856ea91ec1eab445",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-29 19:21.13 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_1000.pt\n",
      "2022-12-29 19:21.13 [info     ] DQN_online_20221229192113: epoch=1 step=1000 epoch=1 metrics={'time_inference': 0.0002002553939819336, 'time_environment_step': 1.322793960571289e-05, 'time_step': 0.00022890019416809082, 'rollout_return': 14.271428571428572, 'evaluation': 9.4} step=1000\n",
      "2022-12-29 19:21.15 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_2000.pt\n",
      "2022-12-29 19:21.15 [info     ] DQN_online_20221229192113: epoch=2 step=2000 epoch=2 metrics={'time_inference': 0.00022844457626342775, 'time_environment_step': 1.7648935317993165e-05, 'time_sample_batch': 5.428791046142578e-05, 'time_algorithm_update': 0.0011993114948272705, 'loss': 0.20065757550066338, 'time_step': 0.001529106855392456, 'rollout_return': 32.733333333333334, 'evaluation': 101.3} step=2000\n",
      "2022-12-29 19:21.17 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_3000.pt\n",
      "2022-12-29 19:21.17 [info     ] DQN_online_20221229192113: epoch=3 step=3000 epoch=3 metrics={'time_inference': 0.00023581194877624512, 'time_environment_step': 1.7439842224121093e-05, 'time_sample_batch': 5.3737640380859374e-05, 'time_algorithm_update': 0.0012964718341827392, 'loss': 0.2543367713317275, 'time_step': 0.0016323039531707763, 'rollout_return': 87.45454545454545, 'evaluation': 115.5} step=3000\n",
      "2022-12-29 19:21.19 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_4000.pt\n",
      "2022-12-29 19:21.19 [info     ] DQN_online_20221229192113: epoch=4 step=4000 epoch=4 metrics={'time_inference': 0.0002613496780395508, 'time_environment_step': 1.9296646118164064e-05, 'time_sample_batch': 6.102323532104492e-05, 'time_algorithm_update': 0.00145501446723938, 'loss': 0.2036749088652432, 'time_step': 0.0018291232585906983, 'rollout_return': 143.85714285714286, 'evaluation': 148.2} step=4000\n",
      "2022-12-29 19:21.21 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_5000.pt\n",
      "2022-12-29 19:21.21 [info     ] DQN_online_20221229192113: epoch=5 step=5000 epoch=5 metrics={'time_inference': 0.00027924656867980954, 'time_environment_step': 2.131366729736328e-05, 'time_sample_batch': 6.777310371398926e-05, 'time_algorithm_update': 0.0015572540760040283, 'loss': 0.16964129781548398, 'time_step': 0.00196170973777771, 'rollout_return': 124.875, 'evaluation': 161.4} step=5000\n",
      "2022-12-29 19:21.23 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_6000.pt\n",
      "2022-12-29 19:21.23 [info     ] DQN_online_20221229192113: epoch=6 step=6000 epoch=6 metrics={'time_inference': 0.0002794873714447022, 'time_environment_step': 2.0830631256103517e-05, 'time_sample_batch': 6.69405460357666e-05, 'time_algorithm_update': 0.0015494918823242187, 'loss': 0.3019486553086899, 'time_step': 0.0019515366554260253, 'rollout_return': 150.0, 'evaluation': 136.3} step=6000\n",
      "2022-12-29 19:21.26 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_7000.pt\n",
      "2022-12-29 19:21.26 [info     ] DQN_online_20221229192113: epoch=7 step=7000 epoch=7 metrics={'time_inference': 0.00026018810272216797, 'time_environment_step': 2.0025491714477538e-05, 'time_sample_batch': 6.088709831237793e-05, 'time_algorithm_update': 0.001435467004776001, 'loss': 0.33737634381989484, 'time_step': 0.0018102877140045166, 'rollout_return': 187.33333333333334, 'evaluation': 192.5} step=7000\n",
      "2022-12-29 19:21.28 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_8000.pt\n",
      "2022-12-29 19:21.28 [info     ] DQN_online_20221229192113: epoch=8 step=8000 epoch=8 metrics={'time_inference': 0.00028421998023986817, 'time_environment_step': 2.271103858947754e-05, 'time_sample_batch': 7.057356834411621e-05, 'time_algorithm_update': 0.0015230319499969483, 'loss': 0.35268680154276083, 'time_step': 0.0019384565353393554, 'rollout_return': 117.0, 'evaluation': 150.2} step=8000\n",
      "2022-12-29 19:21.30 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_9000.pt\n",
      "2022-12-29 19:21.30 [info     ] DQN_online_20221229192113: epoch=9 step=9000 epoch=9 metrics={'time_inference': 0.0002592184543609619, 'time_environment_step': 1.991128921508789e-05, 'time_sample_batch': 6.107091903686523e-05, 'time_algorithm_update': 0.0014097416400909424, 'loss': 0.8176479408040177, 'time_step': 0.0017831122875213623, 'rollout_return': 137.42857142857142, 'evaluation': 171.6} step=9000\n",
      "2022-12-29 19:21.32 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_10000.pt\n",
      "2022-12-29 19:21.32 [info     ] DQN_online_20221229192113: epoch=10 step=10000 epoch=10 metrics={'time_inference': 0.00024687814712524413, 'time_environment_step': 1.7707109451293947e-05, 'time_sample_batch': 5.719327926635742e-05, 'time_algorithm_update': 0.0014399569034576416, 'loss': 0.7988631286148448, 'time_step': 0.0017916104793548584, 'rollout_return': 128.375, 'evaluation': 160.6} step=10000\n",
      "2022-12-29 19:21.34 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_11000.pt\n",
      "2022-12-29 19:21.34 [info     ] DQN_online_20221229192113: epoch=11 step=11000 epoch=11 metrics={'time_inference': 0.00023188281059265136, 'time_environment_step': 1.692795753479004e-05, 'time_sample_batch': 5.1918745040893556e-05, 'time_algorithm_update': 0.0013553063869476318, 'loss': 0.8038233526365366, 'time_step': 0.001684506893157959, 'rollout_return': 128.875, 'evaluation': 200.0} step=11000\n",
      "2022-12-29 19:21.36 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_12000.pt\n",
      "2022-12-29 19:21.36 [info     ] DQN_online_20221229192113: epoch=12 step=12000 epoch=12 metrics={'time_inference': 0.00023354291915893554, 'time_environment_step': 1.7008066177368165e-05, 'time_sample_batch': 5.3325891494750975e-05, 'time_algorithm_update': 0.0013843216896057128, 'loss': 0.32295891388028397, 'time_step': 0.001716744899749756, 'rollout_return': 186.4, 'evaluation': 182.9} step=12000\n",
      "2022-12-29 19:21.38 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_13000.pt\n",
      "2022-12-29 19:21.38 [info     ] DQN_online_20221229192113: epoch=13 step=13000 epoch=13 metrics={'time_inference': 0.00023905301094055174, 'time_environment_step': 1.7758846282958985e-05, 'time_sample_batch': 5.490350723266602e-05, 'time_algorithm_update': 0.0014129538536071778, 'loss': 0.712730813348724, 'time_step': 0.0017546570301055907, 'rollout_return': 145.14285714285714, 'evaluation': 172.5} step=13000\n",
      "2022-12-29 19:21.40 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_14000.pt\n",
      "2022-12-29 19:21.40 [info     ] DQN_online_20221229192113: epoch=14 step=14000 epoch=14 metrics={'time_inference': 0.00027175641059875487, 'time_environment_step': 2.020406723022461e-05, 'time_sample_batch': 6.3018798828125e-05, 'time_algorithm_update': 0.001530190944671631, 'loss': 0.9298542860498419, 'time_step': 0.0019197142124176025, 'rollout_return': 141.42857142857142, 'evaluation': 152.7} step=14000\n",
      "2022-12-29 19:21.42 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_15000.pt\n",
      "2022-12-29 19:21.42 [info     ] DQN_online_20221229192113: epoch=15 step=15000 epoch=15 metrics={'time_inference': 0.0002615799903869629, 'time_environment_step': 1.9949436187744142e-05, 'time_sample_batch': 6.232142448425293e-05, 'time_algorithm_update': 0.001505129337310791, 'loss': 0.9489821713729761, 'time_step': 0.0018829238414764405, 'rollout_return': 134.25, 'evaluation': 151.4} step=15000\n",
      "2022-12-29 19:21.45 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_16000.pt\n",
      "2022-12-29 19:21.45 [info     ] DQN_online_20221229192113: epoch=16 step=16000 epoch=16 metrics={'time_inference': 0.00027002692222595216, 'time_environment_step': 2.0896434783935547e-05, 'time_sample_batch': 6.48355484008789e-05, 'time_algorithm_update': 0.0015522356033325195, 'loss': 0.9104395122918068, 'time_step': 0.0019428889751434326, 'rollout_return': 161.33333333333334, 'evaluation': 200.0} step=16000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-29 19:21.47 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_17000.pt\n",
      "2022-12-29 19:21.47 [info     ] DQN_online_20221229192113: epoch=17 step=17000 epoch=17 metrics={'time_inference': 0.0002535417079925537, 'time_environment_step': 1.930856704711914e-05, 'time_sample_batch': 6.063437461853027e-05, 'time_algorithm_update': 0.0014595577716827392, 'loss': 0.28266326214402215, 'time_step': 0.0018254013061523436, 'rollout_return': 196.0, 'evaluation': 183.7} step=17000\n",
      "2022-12-29 19:21.49 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_18000.pt\n",
      "2022-12-29 19:21.49 [info     ] DQN_online_20221229192113: epoch=18 step=18000 epoch=18 metrics={'time_inference': 0.0002378098964691162, 'time_environment_step': 1.804494857788086e-05, 'time_sample_batch': 5.429720878601074e-05, 'time_algorithm_update': 0.0013373541831970216, 'loss': 0.1859688821757445, 'time_step': 0.0016769711971282959, 'rollout_return': 197.4, 'evaluation': 195.5} step=18000\n",
      "2022-12-29 19:21.51 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_19000.pt\n",
      "2022-12-29 19:21.51 [info     ] DQN_online_20221229192113: epoch=19 step=19000 epoch=19 metrics={'time_inference': 0.0002474837303161621, 'time_environment_step': 1.8099784851074218e-05, 'time_sample_batch': 5.757689476013184e-05, 'time_algorithm_update': 0.0013958144187927247, 'loss': 0.16536722024640768, 'time_step': 0.001749448299407959, 'rollout_return': 181.83333333333334, 'evaluation': 193.0} step=19000\n",
      "2022-12-29 19:21.53 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_20000.pt\n",
      "2022-12-29 19:21.53 [info     ] DQN_online_20221229192113: epoch=20 step=20000 epoch=20 metrics={'time_inference': 0.00023467516899108886, 'time_environment_step': 1.7293930053710938e-05, 'time_sample_batch': 5.4136991500854494e-05, 'time_algorithm_update': 0.0013649449348449706, 'loss': 0.500081924961065, 'time_step': 0.0017002298831939697, 'rollout_return': 157.83333333333334, 'evaluation': 192.9} step=20000\n",
      "2022-12-29 19:21.55 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_21000.pt\n",
      "2022-12-29 19:21.55 [info     ] DQN_online_20221229192113: epoch=21 step=21000 epoch=21 metrics={'time_inference': 0.0002545483112335205, 'time_environment_step': 1.8922567367553712e-05, 'time_sample_batch': 6.0712099075317385e-05, 'time_algorithm_update': 0.0014917314052581786, 'loss': 0.5177039819431957, 'time_step': 0.0018580985069274903, 'rollout_return': 182.0, 'evaluation': 200.0} step=21000\n",
      "2022-12-29 19:21.57 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_22000.pt\n",
      "2022-12-29 19:21.57 [info     ] DQN_online_20221229192113: epoch=22 step=22000 epoch=22 metrics={'time_inference': 0.00024694132804870607, 'time_environment_step': 1.8517017364501952e-05, 'time_sample_batch': 5.711245536804199e-05, 'time_algorithm_update': 0.0014118621349334718, 'loss': 0.4248078874370549, 'time_step': 0.0017657217979431153, 'rollout_return': 166.57142857142858, 'evaluation': 200.0} step=22000\n",
      "2022-12-29 19:21.59 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_23000.pt\n",
      "2022-12-29 19:21.59 [info     ] DQN_online_20221229192113: epoch=23 step=23000 epoch=23 metrics={'time_inference': 0.0002575812339782715, 'time_environment_step': 1.8944978713989257e-05, 'time_sample_batch': 6.150364875793456e-05, 'time_algorithm_update': 0.0014392974376678466, 'loss': 0.5287668166456279, 'time_step': 0.0018096530437469482, 'rollout_return': 156.16666666666666, 'evaluation': 200.0} step=23000\n",
      "2022-12-29 19:22.01 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_24000.pt\n",
      "2022-12-29 19:22.01 [info     ] DQN_online_20221229192113: epoch=24 step=24000 epoch=24 metrics={'time_inference': 0.0002493319511413574, 'time_environment_step': 1.798534393310547e-05, 'time_sample_batch': 5.881047248840332e-05, 'time_algorithm_update': 0.0014286530017852783, 'loss': 0.13785015988454688, 'time_step': 0.0017853488922119141, 'rollout_return': 200.0, 'evaluation': 200.0} step=24000\n",
      "2022-12-29 19:22.04 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_25000.pt\n",
      "2022-12-29 19:22.04 [info     ] DQN_online_20221229192113: epoch=25 step=25000 epoch=25 metrics={'time_inference': 0.0002509498596191406, 'time_environment_step': 1.8026113510131836e-05, 'time_sample_batch': 6.000828742980957e-05, 'time_algorithm_update': 0.00144527006149292, 'loss': 0.0032178056008124256, 'time_step': 0.0018050527572631835, 'rollout_return': 200.0, 'evaluation': 200.0} step=25000\n",
      "2022-12-29 19:22.06 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_26000.pt\n",
      "2022-12-29 19:22.06 [info     ] DQN_online_20221229192113: epoch=26 step=26000 epoch=26 metrics={'time_inference': 0.00025340843200683595, 'time_environment_step': 1.8503665924072265e-05, 'time_sample_batch': 6.140780448913574e-05, 'time_algorithm_update': 0.0015010592937469483, 'loss': 0.004722337869228795, 'time_step': 0.0018652560710906982, 'rollout_return': 200.0, 'evaluation': 200.0} step=26000\n",
      "2022-12-29 19:22.08 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_27000.pt\n",
      "2022-12-29 19:22.08 [info     ] DQN_online_20221229192113: epoch=27 step=27000 epoch=27 metrics={'time_inference': 0.00023813176155090333, 'time_environment_step': 1.722574234008789e-05, 'time_sample_batch': 5.379843711853027e-05, 'time_algorithm_update': 0.0013865382671356201, 'loss': 0.0033070841919397936, 'time_step': 0.0017244374752044677, 'rollout_return': 200.0, 'evaluation': 200.0} step=27000\n",
      "2022-12-29 19:22.10 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_28000.pt\n",
      "2022-12-29 19:22.10 [info     ] DQN_online_20221229192113: epoch=28 step=28000 epoch=28 metrics={'time_inference': 0.0002514357566833496, 'time_environment_step': 1.8164873123168944e-05, 'time_sample_batch': 6.079697608947754e-05, 'time_algorithm_update': 0.001470881223678589, 'loss': 0.0037547087660059334, 'time_step': 0.0018318297863006592, 'rollout_return': 200.0, 'evaluation': 200.0} step=28000\n",
      "2022-12-29 19:22.12 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_29000.pt\n",
      "2022-12-29 19:22.12 [info     ] DQN_online_20221229192113: epoch=29 step=29000 epoch=29 metrics={'time_inference': 0.00023711252212524415, 'time_environment_step': 1.736283302307129e-05, 'time_sample_batch': 5.3653240203857424e-05, 'time_algorithm_update': 0.0013738579750061034, 'loss': 0.0025440567938494496, 'time_step': 0.0017107441425323487, 'rollout_return': 200.0, 'evaluation': 200.0} step=29000\n",
      "2022-12-29 19:22.14 [info     ] Model parameters are saved to d3rlpy_logs/DQN_online_20221229192113/model_30000.pt\n",
      "2022-12-29 19:22.14 [info     ] DQN_online_20221229192113: epoch=30 step=30000 epoch=30 metrics={'time_inference': 0.0002510280609130859, 'time_environment_step': 1.7875909805297852e-05, 'time_sample_batch': 5.737709999084473e-05, 'time_algorithm_update': 0.001457993507385254, 'loss': 0.0030505477344268, 'time_step': 0.0018143188953399659, 'rollout_return': 200.0, 'evaluation': 200.0} step=30000\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create an instance of the DQNTrainer class\n",
    "trainer = DQNTrainer(\n",
    "    env_name=\"CartPole-v0\",\n",
    "    eval_env_name=\"CartPole-v0\",\n",
    "    buffer_maxlen=1000,\n",
    "    start_epsilon=1.0,\n",
    "    end_epsilon=0.1,\n",
    "    duration=1000,\n",
    "    batch_size=32,\n",
    "    learning_rate=2.5e-4,\n",
    "    target_update_interval=100,\n",
    "    optim_factory=OptimizerFactory(Adam, weight_decay=1e-4),\n",
    "    n_steps=30000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    update_start_step=1000\n",
    ")\n",
    "\n",
    "# start training\n",
    "trainer.train()\n",
    "score = trainer.evaluate()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c072b1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
